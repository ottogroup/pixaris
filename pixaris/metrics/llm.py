from concurrent.futures import ThreadPoolExecutor
import json
import re
import io
from PIL.Image import Image
import vertexai
from vertexai.generative_models import GenerativeModel, Image as GenAIImage, Part
import yaml
from pixaris.metrics.base import BaseMetric
from pixaris.metrics.utils import dict_mean
from pixaris.utils.retry import retry


class BaseLLMMetric(BaseMetric):
    """
    BaseLLMMetric is a base class for metrics that use a Gemini large language model (LLM) to evaluate images.

    :param prompt: The prompt string for the LLM. The prompt has to make sure what should be evaluated and that
        the output is JSON formatted. '{"base_llm_metric": x}' where x is the score. Can also be multiple scores in one.
    :type prompt: str
    :param sample_size: The number of times to call the LLM for the same image. Defaults to 3.
    :type sample_size: int, optional
    :param reference_images: A dictionary of reference images.
    :type reference_images: dict[str, list[Image]]
    """

    def __init__(
        self, prompt: str, sample_size: int = 3, **reference_images: list[Image]
    ):
        super().__init__()
        self.prompt = prompt
        self.sample_size = sample_size
        self.reference_images = reference_images

    def _verify_input_images(self, input_images: list[Image]):
        """
        Verify that the input images are valid and that the number of input images matches the number of reference images.
        """
        if not isinstance(input_images, list):
            raise ValueError("Input images must be a list.")
        if not all(isinstance(image, Image) for image in input_images):
            raise ValueError("All input images must be PIL Image objects.")

        input_image_len = len(input_images)

        for image_list in self.reference_images.values():
            if not isinstance(image_list, list):
                raise ValueError("Reference images must be a list.")
            if not all(isinstance(image, Image) for image in image_list):
                raise ValueError("All reference images must be PIL Image objects.")
            if len(image_list) != input_image_len:
                raise ValueError(
                    f"Number of reference images ({len(image_list)}) does not match number of input images ({input_image_len})."
                )

    @retry(exceptions=Exception, tries=3, delay=0.5, max_delay=2, backoff=2)
    def _PIL_image_to_vertex_image(self, image: Image) -> GenAIImage:
        """
        Converts a PIL image to a vertex image.

        :param image: The PIL image.
        :type image: PIL.Image.Image
        :return: The vertex image.
        :rtype: vertexai.generative_models.Image
        """
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format="PNG")
        return GenAIImage.from_bytes(img_byte_arr.getvalue())

    def _llm_prompt(
        self,
        json_prompt: str,
        images: list[Image],
    ):
        """
        Generates a prompt for the LLM using the provided JSON prompt and images.

        :param json_prompt: The JSON prompt string.
        :type json_prompt: str
        :param images: A list of images to include in the prompt.
        :type images: list[Image]
        :return: A list containing the JSON prompt and the images as vertexai.generative_models.Part objects.
        :rtype: list[vertexai.generative_models.Part, str]
        """
        return [
            json_prompt,
            *[
                Part.from_image(self._PIL_image_to_vertex_image(image))
                for image in images
            ],
        ]

    def _postprocess_response(self, response_text: str) -> str:
        """
        If there is some sort of JSON-like structure in the response text, extract it and return it.

        :param response_text: The response text from the model.
        :type response_text: str
        :return: The extracted JSON-like structure if found, otherwise the original response text.
        :rtype: str
        """
        pattern = r"\{.*?\}"
        match = re.search(pattern, response_text, re.DOTALL)
        if match:
            extracted_string = match.group(0)
            return extracted_string
        else:
            raise ValueError("No JSON-like structure found in the llm response text.")

    def _response_to_dict(self, response_text: str) -> dict:
        """
        Converts the response text to a dictionary.

        :param response_text: The response text from the model.
        :type response_text: str
        :return: The response text as a dictionary.
        :rtype: dict
        """
        parsed_text = self._postprocess_response(response_text)
        return json.loads(parsed_text)

    def _call_gemini(self, prompt: list[vertexai.generative_models.Part, str]) -> str:
        """
        Sends the prompt to Google API

        :param prompt: The prompt for the LLM metrics. Generated by _llm_prompt().
        :type prompt: list[vertexai.generative_models.Part, str]
        :return: The LLM response.
        :rtype: str
        """
        with open("pixaris/config.yaml", "r") as f:
            config = yaml.safe_load(f)

        vertexai.init(project=config["gcp_project_id"], location=config["gcp_location"])

        model = GenerativeModel("gemini-2.0-flash")

        responses = model.generate_content(prompt, stream=False)

        return responses.text

    def _successful_evaluation(
        self, prompt: list[vertexai.generative_models.Part, str], max_tries: int = 3
    ) -> dict:
        """
        Perform an evaluation by calling the `_call_gemini` function with the given parameters.
        Assures that gemini returns correct json code by calling it up to max_tries times if it fails.

        :param prompt: The prompt for the LLM metrics. Generated by llm_prompt().
        :type prompt: list[vertexai.generative_models.Part, str]
        :param max_tries: The maximum number of tries to perform the LLM call. Defaults to 3.
        :type max_tries: int
        :return: The LLM response as a dictionary.
        :rtype: dict
        :raises ValueError: If the response cannot be parsed as JSON.
        """
        for i in range(max_tries):
            try:
                ans = self._response_to_dict(self._call_gemini(prompt))
                return ans
            except ValueError:
                pass

    def _llm_scores_per_image(
        self,
        evaluation_image: Image,
        *reference_images: list[Image],
    ) -> dict:
        """
        Calculates the LLM score for the generated image and possibly some reference images.

        :param evaluation_image: The generated image.
        :type evaluation_image: PIL.Image.Image
        :param reference_images: A list of reference images.
        :type reference_images: list[Image]
        :return: A dictionary containing the LLM scores for the evaluation image.
        :rtype: dict
        """
        scores = [
            self._successful_evaluation(
                self._llm_prompt(self.prompt, [evaluation_image, *reference_images]),
            )
            for _ in range(self.sample_size)
        ]

        # Calculate the average score for each metric
        average_scores_per_metric = dict_mean(scores)

        return average_scores_per_metric

    def calculate(self, evaluation_images: list[Image]) -> dict:
        """
        Calculate the LLM metrics for a list of evaluation images.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        :raises ValueError: If the number of evaluation images does not match the number of reference images.
        """
        self._verify_input_images(evaluation_images)

        with ThreadPoolExecutor(len(evaluation_images)) as executor:
            llm_metrics = dict_mean(
                list(
                    executor.map(
                        self._llm_scores_per_image,
                        evaluation_images,
                        *self.reference_images.values(),
                    )
                )
            )
            return llm_metrics


class SimilarityLLMMetric(BaseLLMMetric):
    """
    SimilarityLLMMetric is a subclass of BaseLLMMetric that uses a Gemini LLM to evaluate the similarity between images.

    :param reference_images: A list of reference images to compare against.
    :type reference_images: list[Image]
    """

    def __init__(self, reference_images: list[Image]):
        """
        Initialize the SimilarityLLMMetric.

        :param reference_images: A list of reference images to compare against.
        :type reference_images: dict[str, list[Image]]
        """
        prompt = (
            "The prompt contains two photos: (1) The product photo that was taken in the "
            "studio. (2) An AI generated photo that puts the product into a scene. "
            "The AI generated photo has the tendency to modify the product image "
            "which is not desired. You are a QA expert that reviews the AI generated picture "
            "and compares how close the depicted product matches the original "
            "product photo. You give an overall match score between 0 and 1. To calculate the score you "
            "evaluate the match of (a) colors, (b) geometries, (c) textures, (d) details. Use JSON as output: "
            "{'similarity_llm_metric': x} where the likelihood x"
            "of a match, is expressed as an float percentage between 0 and 1. 1 should be a "
            "perfect match and 0 should be a complete mismatch. "
        )
        super().__init__(
            prompt=prompt,
            reference_images=reference_images,
        )


class StyleLLMMetric(BaseLLMMetric):
    """
    StyleLLMMetric is a subclass of BaseLLMMetric that uses a Gemini LLM to evaluate the style of images.

    :param object_images: A list of object images to compare against.
    :type object_images: list[Image]
    :param style_images: A list of style images to compare against.
    :type style_images: list[Image]
    """

    def __init__(self, **reference_images: list[Image]):
        """
        Initialize the StyleLLMMetric.

        :param reference_images: A dictionary of reference images.
        :type reference_images: dict[str, list[Image]]
        """
        super().__init__(
            prompt="""
                **Objective:** Analyze the provided image(s) and generate a detailed textual style guide capturing their 
                core visual characteristics. This style guide will be used later to generate new background images in a 
                similar style, specifically for a retail context.

                **Analyze the following aspects of the provided image(s):**

                1.  **Overall Mood & Atmosphere:**
                    * Describe the general feeling (e.g., minimalist, luxurious, cozy, energetic, futuristic, rustic, playful, sophisticated, calm).
                    * Is it bright and airy, dark and moody, dramatic, subdued?

                2.  **Color Palette:**
                    * Identify the dominant colors.
                    * Identify key accent colors.
                    * Describe the color relationships (e.g., monochromatic, analogous, complementary, triadic).
                    * Characterize the saturation (vibrant, muted, desaturated).
                    * Characterize the brightness/value (light, dark, high contrast, low contrast).

                3.  **Lighting:**
                    * Describe the type of lighting (e.g., natural, artificial, studio, ambient).
                    * Identify the light direction (e.g., front-lit, side-lit, back-lit, top-down).
                    * Describe the quality of light (e.g., hard and sharp, soft and diffused).
                    * Characterize the shadows (e.g., deep, soft, minimal, dramatic).
                    * Is there a specific lighting effect (e.g., lens flare, bokeh, glow)?

                4.  **Composition & Framing:**
                    * Are there strong compositional principles used (e.g., rule of thirds, symmetry, asymmetry, leading lines, golden ratio)?
                    * Describe the depth of field (shallow with bokeh, deep focus).
                    * Is the framing tight or wide?
                    * How is negative space used?
                    * What is the typical camera angle or perspective (e.g., eye-level, low angle, high angle)?

                5.  **Textures & Materials:**
                    * Describe prominent textures (e.g., smooth, rough, glossy, matte, metallic, wooden, fabric, concrete).
                    * Identify any specific materials that define the style.
                    * Is the overall feel clean or textured?

                6.  **Key Elements & Motifs (if applicable):**
                    * Are there recurring shapes, patterns, objects, or graphic elements that define the style?
                    * Is there a particular theme (e.g., nature, technology, urban)?

                7.  **Level of Detail & Complexity:**
                    * Is the style detailed and intricate, or simple and clean?
                    * Is it photorealistic, illustrative, abstract, painterly?

                **Output Format:**
                Please provide the analysis as a structured style guide, using clear headings or bullet points for each 
                category listed above. Be descriptive and specific.
                Do not add any additional information, but provide the output right away without any introductory text.
                """,
        )
        self.reference_images = reference_images

    def _describe_images(self, images: list[list[Image]]) -> list[str]:
        """
        Describe the images using the style extraction prompt.
        images is a list of lists. All te n-th images will be described together.
        Example::
            images = [[image1, image2], [image3, image4]]
            will describe image1 and image3 together and image2 and image4 together.

        :param images: A list of list images to describe.
        :type images: list[list[Image]]
        :return: A list of descriptions for the reference images.
        :rtype: list[str]
        """
        # Transpose the list of lists to group images by index
        grouped_images = list(zip(*images))
        prompts = [
            self._llm_prompt(self.prompt, list(images)) for images in grouped_images
        ]
        with ThreadPoolExecutor(len(self.reference_images)) as executor:
            descriptions = list(
                executor.map(
                    self._call_gemini,
                    prompts,
                )
            )
            return descriptions

    def _compare_images_to_descriptions(
        self,
        evaluation_images: list[Image],
        reference_image_descriptions: list[str],
    ) -> dict:
        """
        Compare the evaluation images to the reference image descriptions.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :param reference_image_descriptions: A list of reference image descriptions.
        :type reference_image_descriptions: list[str]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        """
        comparison_prompt = """
        You are an expert image analysis AI. Your task is to meticulously compare the visual style of the provided [IMAGE] against a detailed textual [DESCRIPTION] of an image's style.

        Objective:
        For each distinct stylistic attribute or aspect mentioned in the [DESCRIPTION], you will evaluate how accurately and consistently the [IMAGE] visually embodies that attribute.

        Scoring Guide:
        Assign a numeric score between 0.0 and 1.0 (inclusive) for each point:

        1.0: Perfect, unequivocal match. The image completely and obviously displays this attribute as described.
        0.8 - 0.9: Strong match with minor, negligible deviations or slight ambiguity.
        0.6 - 0.7: Moderate match. The attribute is present but might have noticeable differences, be less prominent, or only partially apply.
        0.3 - 0.5: Weak match. There are some similarities, but significant discrepancies or outright mismatches.
        0.0 - 0.2: No discernible match. The image contradicts the description for this attribute or it is entirely absent.
        Output Format:
        Your output must be a single JSON object.
        Iterate through the [DESCRIPTION] sequentially. Identify each distinct, actionable stylistic statement (each bullet point or sub-bullet point represents a distinct statement).
        Assign a key style_X where X is an incrementing integer starting from 1 for the very first statement and increasing by 1 for each subsequent statement in the [DESCRIPTION]. The value for each key should be the calculated float score.
        The keys are numbered from 1 to seven, so the output keys should alsop be numbered from 1 to 7.
        
        Do NOT include any introductory or concluding text, explanations, or conversational remarks. Output ONLY the JSON object.

        Example of how to number the statements for JSON keys:

        Given this fragment from the description:

        **Analyzing Retail Backgrounds**

        **1. Overall Mood & Atmosphere:**
        *   **General Feeling:** Whimsical, charming, playful, and clean. There's a subtle sophistication balanced with an endearing cuteness.
        *   **Brightness/Atmosphere:** Bright and airy, dominated by the luminous light source in the background, creating an uplifting and optimistic feel.

        **2. Color Palette:**
        *   **Dominant Colors:**
            *   **Background:** Vibrant and lush greens (lime green, emerald green, forest green), creating a natural, leafy forest ambiance.
            *   **Subject:** Dominant greys (various shades from light silver to charcoal) for the chinchilla's fur, pure white for the shirt collar, and light beige/tan for the ground.
        *   **Key Accent Colors:** A strong, saturated red for the bow tie, providing a striking focal point against the greens and greys.
        *   **Color Relationships:** A harmonious blend of analogous greens in the background, contrasted by complementary greys and white in the foreground. The red bow tie acts as a highly effective accent, creating a strong visual pop.
        *   **Saturation:** The greens are moderately to highly saturated, while the red is vivid. The chinchilla's fur exhibits a natural, desaturated grey.
        *   **Brightness/Value:** High contrast, particularly between the bright, almost blown-out light source and the surrounding greens. The foreground subject is well-lit with clear, defined values, creating depth.

        **3. Lighting:**
        *   **Type of Lighting:** Appears to be a stylized form of natural light, mimicking a sunburst or strong overhead sunlight. It could be a composite of natural background lighting and studio-like lighting on the subject.
        *   **Light Direction:** Predominantly back-lit from the upper center, creating a strong glow and subtle halo effect around the chinchilla's head. The subject itself is illuminated by a softer, more diffused front or ambient light, ensuring all details are visible.
        *   **Quality of Light:** The background light is bright, intense, and slightly diffused creating a glow. The light on the subject is soft and smooth, highlighting textures without harsh reflections or shadows.
        *   **Shadows:** Shadows are minimal and soft, primarily providing subtle dimension under the subject and within its fur. There are no harsh or dramatic shadows.
        *   **Specific Lighting Effect:** A prominent sunburst/light ray effect emanating from the top-center in the background.

        **4. Composition & Framing:**
        *   **Compositional Principles:** Strong central framing, placing the main subject (chinchilla) front and center. The overall composition is vertically oriented, emphasizing the height of the subject and background trees.
        *   **Depth of Field:** Shallow depth of field, with the chinchilla in crisp focus and the background heavily blurred (bokeh effect), effectively isolating the subject and drawing immediate attention to it.
        *   **Framing:** A mid-shot/full-body shot of the chinchilla, occupying a significant portion of the vertical frame.
        *   **Negative Space:** The blurred green foliage serves as effective and simple negative space, enhancing the focus on the subject.
        *   **Camera Angle/Perspective:** Eye-level or slightly low-angle perspective, giving the animal a dignified and engaging presence.

        **5. Textures & Materials:**
        *   **Prominent Textures:**
            *   **Subject:** Ultra-soft, fluffy, and dense fur on the chinchilla; smooth, crisp fabric (like cotton or satin) for the white shirt collar and red bow tie.
            *   **Background:** Heavily blurred organic textures suggestive of leaves, foliage, and tree trunks, providing a natural yet indistinct backdrop.
            *   **Ground:** A subtly textured, smooth, light-colored surface (possibly concrete or a paved path).
        *   **Materials:** Fur, polished fabric (bow tie), cotton/linen (shirt collar), natural outdoor elements (trees, leaves), paving/ground material.
        *   **Overall Feel:** Clean and soft in the foreground; organic and blurred in the background.

        **6. Key Elements & Motifs:**
        *   **Recurring elements:** Anthropomorphic animals (specifically small, cute creatures), dressed in formal or semi-formal attire (bow ties, collars), set against lush, natural outdoor environments. The strong, central light source is also a motif.
        *   **Theme:** Whimsical, light-hearted natural elegance with a touch of character.

        **7. Level of Detail & Complexity:**
        *   **Detail:** High level of detail on the primary subject, capturing intricate fur texture and facial features. The background is intentionally simplistic due to heavy blurring.
        *   **Complexity:** Simple and clean composition, typically featuring a single main character. The overall scene is not cluttered.
        *   **Style:** Photorealistic for the animal subject, subtly combined with an almost painterly or illustrative quality for the blurred background and stylized lighting. It appears to be a high-quality digital render or composite image rather than a raw photograph.
        
        The JSON output should look like this:
        
        {"style_1": 0.9, "style_2": 0.8, "style_3": 0.7, "style_4": 0.6, "style_5": 0.5, "style_6": 0.4, "style_7": 0.3}
        
        The keys are numbered from 1 to seven, the floating point numbers are example. You should find fitting values to describe the similarities in the [DESCRIPTION] and the [IMAGE]. 
        Do not include any formatting like markdown or any other text. Just the JSON output.
        """
        image_parts = [
            Part.from_image(self._PIL_image_to_vertex_image(image))
            for image in evaluation_images
        ]
        prompts = [
            [comparison_prompt, image_part, description]
            for image_part, description in zip(
                image_parts, reference_image_descriptions
            )
        ]

        with ThreadPoolExecutor(len(evaluation_images)) as executor:
            responses = list(
                executor.map(
                    self._call_gemini,
                    prompts,
                )
            )

        return responses

    def _get_mean_style_metric(self, responses: list[str]) -> dict:
        """
        Calculate the mean style metric from the cleaned responses.

        :param cleaned_responses: A list of cleaned responses from the LLM. Return value of _compare_images_to_descriptions.
        :type cleaned_responses: list[str]
        :return: A dictionary containing the mean style metric.
        :rtype: dict
        """
        # Parse the responses and calculate the overall mean value
        parsed_responses = [
            self._response_to_dict(response_text) for response_text in responses
        ]
        mean_responses = dict_mean(parsed_responses)
        overall_average = (
            sum(mean_responses.values()) / len(mean_responses) if mean_responses else 0
        )
        return {
            "style_llm_metric": overall_average,
        }

    def calculate(self, evaluation_images: list[Image]) -> dict:
        """
        Calculate the LLM metrics for a list of evaluation images.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        :raises ValueError: If the number of evaluation images does not match the number of reference images.
        """
        self._verify_input_images(evaluation_images)

        reference_image_descriptions = self._describe_images(
            self.reference_images.values()
        )
        comparison_results = self._compare_images_to_descriptions(
            evaluation_images,
            reference_image_descriptions,
        )
        return self._get_mean_style_metric(comparison_results)


class ErrorLLMMetric(BaseLLMMetric):
    """
    ErrorLLMMetric is a subclass of BaseLLMMetric that uses a Gemini LLM to evaluate the error between images.

    :param object_images: A list of object images to compare against.
    :type object_images: list[Image]
    :param style_images: A list of style images to compare against.
    :type style_images: list[Image]
    """

    def __init__(self):
        """
        Initialize the ErrorLLMMetric.

        :param reference_images: A dictionary of reference images.
        :type reference_images: dict[str, list[Image]]
        """
        prompt = """
        Role: You are an expert Image Quality Assessment AI, specialized in identifying and quantifying malformations in AI-generated images. Your knowledge base includes an exhaustive taxonomy of common generation errors. Your primary directive is to provide a concise, high-signal assessment.

        Objective:

        Analyze the provided image to determine what elements are present (e.g., humans, animals, specific objects, text).
        For each present element, apply the definition of each specific malformation.
        Assign a metric from 0.0 (catastrophic failure) to 1.0 (perfect, no discernible issues) for each applicable malformation type.
        Scoring Guidelines:

        1.0 (Perfect): Absolutely no discernible malformations of this specific type are present.
        0.7 - 0.9 (Minor/Few): Very minor, subtle, or extremely few instances of this malformation. They might be barely noticeable.
        0.4 - 0.6 (Moderate/Visible): Moderate presence of this malformation. Clearly visible, impacts overall quality.
        0.1 - 0.3 (Severe/Many): Severe presence of this malformation. Highly distorted, prevalent, or significantly impacting.
        0.0 (Catastrophic): Complete or near-complete failure in this specific category. Renders the aspect unrecognizable or fundamentally unusable.
        Strict Output Requirements:

        Flat JSON Structure: The output must be a single-level JSON object.
        Key Naming: Each key name must be a concatenation of the full hierarchy path of the malformation, separated by underscores (_).
        Example: anatomical_malformations_hands_fingers_incorrect_finger_count
        Value Only: The value associated with each key must be only the numerical metric (e.g., 0.8), without any details or nested objects.
        Omission of Non-Applicable Elements: If a higher-level category or a specific sub-category for malformations is not applicable to the content of the image (e.g., no animals present, no text to analyze, no objects, or no elements where a specific error type can manifest), then that specific key-value pair must be entirely omitted from the output JSON. Do not include keys for malformations that cannot possibly occur given the image content. For example, if there are no hands, skip all hands_fingers keys. If there's no text, skip all text_symbol_malformations keys.
        Empty Image: If the image is entirely blank or contains no discernible elements to assess, return an empty JSON object {}.

        Flattened Malformation Definitions (Reference List for AI):

        This list defines what each specific malformation represents. Use these definitions to guide your assessment and score assignment.

        anatomical_malformations_hands_fingers_incorrect_finger_count: Too many or too few fingers (e.g., 6, 7, 3, 1).

        anatomical_malformations_hands_fingers_fused_merged_fingers: Fingers appearing stuck together, lacking distinct separation.

        anatomical_malformations_hands_fingers_displaced_misaligned_fingers: Fingers growing from odd places or pointing in unnatural directions.

        anatomical_malformations_hands_fingers_malformed_joints: Fingers bending unnaturally or at impossible angles.

        anatomical_malformations_hands_fingers_incorrect_proportions: Fingers too long/short, too thick/thin.

        anatomical_malformations_hands_fingers_ambiguous_thumbs: Thumbs missing, strangely shaped, or appearing as other fingers.

        anatomical_malformations_hands_fingers_nails: Missing, strangely shaped, or misplaced fingernails.

        anatomical_malformations_hands_fingers_overall_hand_shape_distortion: Hands appearing blob-like, twisted, or unrecognizable.

        anatomical_malformations_hands_fingers_interaction_with_objects: Fingers not gripping objects naturally, merging with objects.

        anatomical_malformations_faces_facial_features_asymmetry: Eyes, nose, mouth, or ears unevenly placed or differing significantly.

        anatomical_malformations_faces_facial_features_mismatched_malformed_features: Eyes without pupils, multiple pupils; mouths with too many/no teeth; noses flattened/distorted; strange ears.

        anatomical_malformations_faces_facial_features_extra_missing_features: Phantom eyes/mouths/noses, or completely missing essential features.

        anatomical_malformations_faces_facial_features_skin_texture_realism: Plastic-like, unnaturally rough, unnatural skin tones, merging with hair/clothing.

        anatomical_malformations_faces_facial_features_expression: Unnatural, forced, blank, or unsettling 'uncanny valley' expressions.

        anatomical_malformations_faces_facial_features_hair: Merging with background, strange textures, floating strands, unnatural partings.

        anatomical_malformations_faces_facial_features_teeth: Too many/few, misaligned, or strangely shaped, often merging into gums.

        anatomical_malformations_bodies_limbs_proportionality_errors: Limbs too long/short, head too large/small for body, torso disproportionate.

        anatomical_malformations_bodies_limbs_joint_malformations: Bending at impossible angles, missing joints, extra joints.

        anatomical_malformations_bodies_limbs_extra_missing_limbs_appendages: Additional or missing arms, legs, or phantom limbs.

        anatomical_malformations_bodies_limbs_merging_blobbing: Limbs merging into torso, other limbs, or background.

        anatomical_malformations_bodies_limbs_unnatural_poses_posture: Contorted, impossible, or extremely rigid poses.

        anatomical_malformations_bodies_limbs_muscle_definition: Over-exaggerated or completely lacking in places.

        anatomical_malformations_bodies_limbs_nudity_clothing_confusion: Clothing melting into skin, or skin appearing where clothing should be.

        anatomical_malformations_animals_creatures_incorrect_limbs_heads: Too many/few legs, tails, heads, or missing body parts.

        anatomical_malformations_animals_creatures_hybridization: Unintentional mixing of different animal features.

        anatomical_malformations_animals_creatures_distorted_anatomy: Twisted bodies, mangled features, or impossible skeletal structures.

        anatomical_malformations_animals_creatures_unnatural_fur_scale_feather_patterns: Disjointed or chaotic textures.

        anatomical_malformations_animals_creatures_facial_malformations: Similar to human faces, but for animals (e.g., misaligned eyes, strange mouths).

        object_prop_malformations_distortion_deformation_melting_liquefaction: Objects appearing to melt or lose solid form.

        object_prop_malformations_distortion_deformation_stretching_squishing: Objects elongated or compressed unnaturally.

        object_prop_malformations_distortion_deformation_bending_warping: Objects appearing to bend or warp without apparent force.

        object_prop_malformations_distortion_deformation_blobbing_amorphousness: Objects losing distinct edges, appearing shapeless.

        object_prop_malformations_distortion_deformation_incorrect_shape_geometric_malformation: Circles as ovals, squares as trapezoids, etc.

        object_prop_malformations_texture_material_inconsistencies_incorrect_material_properties: Metal looking like plastic, wood as stone.

        object_prop_malformations_texture_material_inconsistencies_texture_blurring_fuzziness: Textures lacking detail or appearing smeared.

        object_prop_malformations_texture_material_inconsistencies_repetitive_textures: Repeating patterns where they shouldn't exist.

        object_prop_malformations_texture_material_inconsistencies_material_merging: Different materials bleeding into each other.

        object_prop_malformations_texture_material_inconsistencies_unnatural_reflectivity_gloss: Surfaces too shiny or dull for their material.

        object_prop_malformations_missing_extra_parts_elements: Incomplete objects (car without wheels), spurious random parts.

        object_prop_malformations_compositional_placement_errors_floating_objects: Objects hovering mid-air without support.

        object_prop_malformations_compositional_placement_errors_merging_with_background_other_objects: Objects blending indistinguishably into surroundings.

        object_prop_malformations_compositional_placement_errors_incorrect_scale: Objects too large or too small relative to environment.

        object_prop_malformations_compositional_placement_errors_inconsistent_perspective: Objects appearing at different vanishing points than scene.

        object_prop_malformations_compositional_placement_errors_clipping_interpenetration: Objects passing through each other unrealistically.

        object_prop_malformations_functionality_logical_inconsistencies: Broken/non-functional objects (door without handle), implausible configurations.

        scene_environment_compositional_malformations_perspective_depth_flatness: Lack of depth, making scene appear 2D.

        scene_environment_compositional_malformations_perspective_depth_distorted_perspective: Incorrect or inconsistent vanishing points.

        scene_environment_compositional_malformations_perspective_depth_incorrect_object_placement_in_depth: Foreground objects appearing behind background elements incorrectly.

        scene_environment_compositional_malformations_lighting_shadows_inconsistent_light_sources: Shadows falling in multiple directions where one source should be.

        scene_environment_compositional_malformations_lighting_shadows_missing_shadows: Objects casting no shadows when they should.

        scene_environment_compositional_malformations_lighting_shadows_unnatural_shadows: Shadows too harsh, too soft, or strangely shaped.

        scene_environment_compositional_malformations_lighting_shadows_incorrect_highlights: Highlights where no light source exists or from impossible angles.

        scene_environment_compositional_malformations_lighting_shadows_over_exposure_under_exposure: Parts of image excessively bright or dark, losing detail.

        scene_environment_compositional_malformations_environmental_coherence_impossible_environments: Indoor bleeding into outdoor, conflicting elements (snow in desert).

        scene_environment_compositional_malformations_environmental_coherence_repetitive_patterns: Tiling effects in large areas (walls, terrain).

        scene_environment_compositional_malformations_environmental_coherence_background_blurring_issues: Inconsistent or incorrect background blur.

        scene_environment_compositional_malformations_overall_composition_aesthetics_awkward_cropping: Subjects cut off at odd points, poor framing.

        scene_environment_compositional_malformations_overall_composition_aesthetics_cluttered_chaotic_scenes: Too many elements, lacking clear focal point.

        scene_environment_compositional_malformations_overall_composition_aesthetics_lack_of_artistic_direction: Image appearing generic, uninspired, lacking mood.

        scene_environment_compositional_malformations_overall_composition_aesthetics_stylistic_inconsistency: Elements generated in different artistic styles within image.

        scene_environment_compositional_malformations_overall_composition_aesthetics_color_palette_issues: Unnatural, unbalanced, or clashing colors.

        text_symbol_malformations_garbled_gibberish_text: Unreadable characters, random squiggles, placeholder text.

        text_symbol_malformations_incorrect_language_characters: Attempting English but producing other alphabets.

        text_symbol_malformations_missing_extra_letters: Words misspelled by addition or omission of letters.

        text_symbol_malformations_distorted_text: Text appearing wavy, stretched, melted, or malformed.

        text_symbol_malformations_improper_placement_adherence: Text floating off surfaces, not conforming to 3D objects.

        text_symbol_malformations_symbolic_misinterpretation: Logos, road signs, etc., rendered inaccurately or distorted.

        artifacts_noise_pixelation_low_resolution: Image appears blocky or blurry due to insufficient resolution.

        artifacts_noise_compression_artifacts: Visible blocks or color banding.

        artifacts_noise_digital_noise_grain: Random speckles, static-like interference.

        artifacts_noise_hallucinations_ghosting: Faint, semi-transparent, or random shapes/colors/ghost-like images.

        artifacts_noise_diffusion_artifacts_blurry_fuzzy_patches: Areas lacking detail or appearing smudged.

        artifacts_noise_diffusion_artifacts_over_smoothing: Loss of texture or detail in areas that should have it.

        artifacts_noise_diffusion_artifacts_patchwork_seams: Visible lines or transitions from generated parts.

        artifacts_noise_diffusion_artifacts_color_bleeding: Colors from one area seeping into an adjacent, unrelated area.

        artifacts_noise_diffusion_artifacts_chromatic_aberration: Fringes of color around high-contrast edges.

        semantic_conceptual_errors_misinterpretation_of_prompt: Image deviates significantly from explicit request.

        semantic_conceptual_errors_loss_of_key_prompt_elements: Ignoring crucial details or adjectives within prompt.

        semantic_conceptual_errors_over_under_interpretation: Too literal or too vague generation based on prompt.

        semantic_conceptual_errors_lack_of_desired_mood_emotion: Image's mood contradicts intent.

        semantic_conceptual_errors_inconsistent_narrative_storytelling: Featuring contradictory elements if implied by prompt.

        semantic_conceptual_errors_word_soup_concept_blending: Incoherent blend from multiple conflicting concepts.

        physics_natural_law_violations_gravity_disregard: Objects floating that should be grounded, liquids defying gravity.

        physics_natural_law_violations_material_property_violations: Water looking solid, fire like smoke, opaque glass.

        physics_natural_law_violations_incorrect_reflect_refractions: Reflections where none should be, or geometrically incorrect.

        physics_natural_law_violations_transparency_opacity_errors: Objects that should be transparent are opaque, or vice-versa.

        physics_natural_law_violations_forces_dynamics: Movement or forces depicted unnaturally (e.g., splashes going wrong way).

        Expected Output Format (JSON Example):

        {
        "anatomical_malformations_hands_fingers_incorrect_finger_count": 0.8,
        "anatomical_malformations_hands_fingers_displaced_misaligned_fingers": 0.5,
        "anatomical_malformations_faces_facial_features_asymmetry": 0.9,
        "object_prop_malformations_compositional_placement_errors_floating_objects": 0.6,
        "scene_environment_compositional_malformations_lighting_shadows_missing_shadows": 0.7,
        "text_symbol_malformations_garbled_gibberish_text": 0.2,
        "artifacts_noise_diffusion_artifacts_blurry_fuzzy_patches": 0.4,
        "semantic_conceptual_errors_misinterpretation_of_prompt": 0.3,
        "physics_natural_law_violations_gravity_disregard": 0.1
        // ... only include keys for applicable malformations ...
        }
        
        Image for Analysis:
        """
        super().__init__(
            prompt=prompt,
        )

    def _get_mean_metric(self, responses: list[dict]) -> float:
        """
        Calculate the mean error metric from the cleaned response.

        :param cleaned_response: A dictionary containing the cleaned response from the LLM.
        :type cleaned_response: dict
        :return: The mean error metric.
        :rtype: float
        """
        # Parse the response and calculate the overall mean value
        mean_value = [
            (sum(response.values()) / len(response) if responses else 1)
            for response in responses
        ]
        return sum(mean_value) / len(mean_value) if mean_value else 1

    def calculate(self, evaluation_images: list[Image]) -> dict:
        """
        Calculate the LLM metrics for a list of evaluation images.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        :raises ValueError: If the number of evaluation images does not match the number of reference images.
        """
        self._verify_input_images(evaluation_images)
        prompts = [
            self._llm_prompt(self.prompt, [evaluation_image])
            for evaluation_image in evaluation_images
        ]

        with ThreadPoolExecutor(len(evaluation_images)) as executor:
            llm_metrics = list(
                executor.map(
                    self._successful_evaluation,
                    prompts,
                )
            )

        mean_metric = self._get_mean_metric(llm_metrics)

        return {"error_llm_metric": mean_metric}
