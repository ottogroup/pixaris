from concurrent.futures import ThreadPoolExecutor
import json
import re
import io
from PIL.Image import Image
import vertexai
from vertexai.generative_models import GenerativeModel, Image as GenAIImage, Part
import yaml
from pixaris.metrics.base import BaseMetric
from pixaris.metrics.utils import dict_mean
from pixaris.utils.retry import retry


class BaseLLMMetric(BaseMetric):
    """
    BaseLLMMetric is a base class for metrics that use a Gemini large language model (LLM) to evaluate images.

    :param prompt: The prompt string for the LLM. The prompt has to make sure what should be evaluated and that
        the output is JSON formatted. '{"base_llm_metric": x}' where x is the score. Can also be multiple scores in one.
    :type prompt: str
    :param sample_size: The number of times to call the LLM for the same image. Defaults to 3.
    :type sample_size: int, optional
    :param reference_images: A dictionary of reference images.
    :type reference_images: dict[str, list[Image]]
    """

    def __init__(
        self, prompt: str, sample_size: int = 3, **reference_images: list[Image]
    ):
        super().__init__()
        self.prompt = prompt
        self.sample_size = sample_size
        self.reference_images = reference_images

    def _verify_input_images(self, input_images: list[Image]):
        """
        Verify that the input images are valid and that the number of input images matches the number of reference images.
        """
        if not isinstance(input_images, list):
            raise ValueError("Input images must be a list.")
        if not all(isinstance(image, Image) for image in input_images):
            raise ValueError("All input images must be PIL Image objects.")

        input_image_len = len(input_images)

        for image_list in self.reference_images.values():
            if not isinstance(image_list, list):
                raise ValueError("Reference images must be a list.")
            if not all(isinstance(image, Image) for image in image_list):
                raise ValueError("All reference images must be PIL Image objects.")
            if len(image_list) != input_image_len:
                raise ValueError(
                    f"Number of reference images ({len(image_list)}) does not match number of input images ({input_image_len})."
                )

    @retry(exceptions=Exception, tries=3, delay=0.5, max_delay=2, backoff=2)
    def _PIL_image_to_vertex_image(self, image: Image) -> GenAIImage:
        """
        Converts a PIL image to a vertex image.

        :param image: The PIL image.
        :type image: PIL.Image.Image
        :return: The vertex image.
        :rtype: vertexai.generative_models.Image
        """
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format="PNG")
        return GenAIImage.from_bytes(img_byte_arr.getvalue())

    def _llm_prompt(
        self,
        json_prompt: str,
        images: list[Image],
    ):
        """
        Generates a prompt for the LLM using the provided JSON prompt and images.

        :param json_prompt: The JSON prompt string.
        :type json_prompt: str
        :param images: A list of images to include in the prompt.
        :type images: list[Image]
        :return: A list containing the JSON prompt and the images as vertexai.generative_models.Part objects.
        :rtype: list[vertexai.generative_models.Part, str]
        """
        return [
            json_prompt,
            *[
                Part.from_image(self._PIL_image_to_vertex_image(image))
                for image in images
            ],
        ]

    def _postprocess_response(self, response_text: str) -> str:
        """
        If there is some sort of JSON-like structure in the response text, extract it and return it.

        :param response_text: The response text from the model.
        :type response_text: str
        :return: The extracted JSON-like structure if found, otherwise the original response text.
        :rtype: str
        """
        pattern = r"\{.*\}"
        match = re.search(pattern, response_text)
        if match:
            extracted_string = match.group(0)
            return extracted_string
        else:
            raise ValueError("No JSON-like structure found in the llm response text.")

    def _response_to_dict(self, response_text: str) -> dict:
        """
        Converts the response text to a dictionary.

        :param response_text: The response text from the model.
        :type response_text: str
        :return: The response text as a dictionary.
        :rtype: dict
        """
        parsed_text = self._postprocess_response(response_text)
        return json.loads(parsed_text)

    def _call_gemini(self, prompt: list[vertexai.generative_models.Part, str]) -> str:
        """
        Sends the prompt to Google API

        :param prompt: The prompt for the LLM metrics. Generated by _llm_prompt().
        :type prompt: list[vertexai.generative_models.Part, str]
        :return: The LLM response.
        :rtype: str
        """
        with open("pixaris/config.yaml", "r") as f:
            config = yaml.safe_load(f)

        vertexai.init(project=config["gcp_project_id"], location=config["gcp_location"])

        model = GenerativeModel("gemini-2.0-flash")

        responses = model.generate_content(prompt, stream=False)

        return responses.text

    def _successful_evaluation(
        self, prompt: list[vertexai.generative_models.Part, str], max_tries: int = 3
    ) -> dict:
        """
        Perform an evaluation by calling the `_call_gemini` function with the given parameters.
        Assures that gemini returns correct json code by calling it up to max_tries times if it fails.

        :param prompt: The prompt for the LLM metrics. Generated by llm_prompt().
        :type prompt: list[vertexai.generative_models.Part, str]
        :param max_tries: The maximum number of tries to perform the LLM call. Defaults to 3.
        :type max_tries: int
        :return: The LLM response as a dictionary.
        :rtype: dict
        :raises ValueError: If the response cannot be parsed as JSON.
        """
        for i in range(max_tries):
            try:
                ans = self._response_to_dict(self._call_gemini(prompt))
                return ans
            except ValueError:
                pass

    def _llm_scores_per_image(
        self,
        evaluation_image: Image,
        *reference_images: list[Image],
    ) -> dict:
        """
        Calculates the LLM score for the generated image and possibly some reference images.

        :param evaluation_image: The generated image.
        :type evaluation_image: PIL.Image.Image
        :param reference_images: A list of reference images.
        :type reference_images: list[Image]
        :return: A dictionary containing the LLM scores for the evaluation image.
        :rtype: dict
        """
        scores = [
            self._successful_evaluation(
                self._llm_prompt(self.prompt, [evaluation_image, *reference_images]),
            )
            for _ in range(self.sample_size)
        ]

        # Calculate the average score for each metric
        average_scores_per_metric = dict_mean(scores)

        return average_scores_per_metric

    def calculate(self, evaluation_images: list[Image]) -> dict:
        """
        Calculate the LLM metrics for a list of evaluation images.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        :raises ValueError: If the number of evaluation images does not match the number of reference images.
        """
        self._verify_input_images(evaluation_images)

        with ThreadPoolExecutor(len(evaluation_images)) as executor:
            llm_metrics = dict_mean(
                list(
                    executor.map(
                        self._llm_scores_per_image,
                        evaluation_images,
                        *self.reference_images.values(),
                    )
                )
            )
            return llm_metrics


class SimilarityLLMMetric(BaseLLMMetric):
    """
    SimilarityLLMMetric is a subclass of BaseLLMMetric that uses a Gemini LLM to evaluate the similarity between images.

    :param reference_images: A list of reference images to compare against.
    :type reference_images: list[Image]
    """

    def __init__(self, reference_images: list[Image]):
        """
        Initialize the SimilarityLLMMetric.

        :param reference_images: A list of reference images to compare against.
        :type reference_images: dict[str, list[Image]]
        """
        prompt = (
            "The prompt contains two photos: (1) The product photo that was taken in the "
            "studio. (2) An AI generated photo that puts the product into a scene. "
            "The AI generated photo has the tendency to modify the product image "
            "which is not desired. You are a QA expert that reviews the AI generated picture "
            "and compares how close the depicted product matches the original "
            "product photo. You give an overall match score between 0 and 1. To calculate the score you "
            "evaluate the match of (a) colors, (b) geometries, (c) textures, (d) details. Use JSON as output: "
            "{'similarity_llm_metric': x} where the likelihood x"
            "of a match, is expressed as an float percentage between 0 and 1. 1 should be a "
            "perfect match and 0 should be a complete mismatch. "
        )
        super().__init__(
            prompt=prompt,
            reference_images=reference_images,
        )


class StyleLLMMetric(BaseLLMMetric):
    """
    StyleLLMMetric is a subclass of BaseLLMMetric that uses a Gemini LLM to evaluate the style of images.

    :param object_images: A list of object images to compare against.
    :type object_images: list[Image]
    :param style_images: A list of style images to compare against.
    :type style_images: list[Image]
    """

    def __init__(self, **reference_images: list[Image]):
        """
        Initialize the StyleLLMMetric.

        :param reference_images: A dictionary of reference images.
        :type reference_images: dict[str, list[Image]]
        """
        super().__init__(
            prompt="""
                **Objective:** Analyze the provided image(s) and generate a detailed textual style guide capturing their 
                core visual characteristics. This style guide will be used later to generate new background images in a 
                similar style, specifically for a retail context.

                **Analyze the following aspects of the provided image(s):**

                1.  **Overall Mood & Atmosphere:**
                    * Describe the general feeling (e.g., minimalist, luxurious, cozy, energetic, futuristic, rustic, playful, sophisticated, calm).
                    * Is it bright and airy, dark and moody, dramatic, subdued?

                2.  **Color Palette:**
                    * Identify the dominant colors.
                    * Identify key accent colors.
                    * Describe the color relationships (e.g., monochromatic, analogous, complementary, triadic).
                    * Characterize the saturation (vibrant, muted, desaturated).
                    * Characterize the brightness/value (light, dark, high contrast, low contrast).

                3.  **Lighting:**
                    * Describe the type of lighting (e.g., natural, artificial, studio, ambient).
                    * Identify the light direction (e.g., front-lit, side-lit, back-lit, top-down).
                    * Describe the quality of light (e.g., hard and sharp, soft and diffused).
                    * Characterize the shadows (e.g., deep, soft, minimal, dramatic).
                    * Is there a specific lighting effect (e.g., lens flare, bokeh, glow)?

                4.  **Composition & Framing:**
                    * Are there strong compositional principles used (e.g., rule of thirds, symmetry, asymmetry, leading lines, golden ratio)?
                    * Describe the depth of field (shallow with bokeh, deep focus).
                    * Is the framing tight or wide?
                    * How is negative space used?
                    * What is the typical camera angle or perspective (e.g., eye-level, low angle, high angle)?

                5.  **Textures & Materials:**
                    * Describe prominent textures (e.g., smooth, rough, glossy, matte, metallic, wooden, fabric, concrete).
                    * Identify any specific materials that define the style.
                    * Is the overall feel clean or textured?

                6.  **Key Elements & Motifs (if applicable):**
                    * Are there recurring shapes, patterns, objects, or graphic elements that define the style?
                    * Is there a particular theme (e.g., nature, technology, urban)?

                7.  **Level of Detail & Complexity:**
                    * Is the style detailed and intricate, or simple and clean?
                    * Is it photorealistic, illustrative, abstract, painterly?

                **Output Format:**
                Please provide the analysis as a structured style guide, using clear headings or bullet points for each 
                category listed above. Be descriptive and specific.
                Do not add any additional information, but provide the output right away without any introductory text.
                """,
        )
        self.reference_images = reference_images

    def _describe_images(self, images: list[list[Image]]) -> list[str]:
        """
        Describe the images using the style extraction prompt.
        images is a list of lists. All te n-th images will be described together.
        Example::
            images = [[image1, image2], [image3, image4]]
            will describe image1 and image3 together and image2 and image4 together.

        :param images: A list of list images to describe.
        :type images: list[list[Image]]
        :return: A list of descriptions for the reference images.
        :rtype: list[str]
        """
        # Transpose the list of lists to group images by index
        grouped_images = list(zip(*images))
        prompts = [
            self._llm_prompt(self.prompt, list(images)) for images in grouped_images
        ]
        with ThreadPoolExecutor(len(self.reference_images)) as executor:
            descriptions = list(
                executor.map(
                    self._call_gemini,
                    prompts,
                )
            )
            return descriptions

    def _compare_images_to_descriptions(
        self,
        evaluation_images: list[Image],
        reference_image_descriptions: list[str],
    ) -> dict:
        """
        Compare the evaluation images to the reference image descriptions.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :param reference_image_descriptions: A list of reference image descriptions.
        :type reference_image_descriptions: list[str]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        """
        comparison_prompt = """
        You are an expert image analysis AI. Your task is to meticulously compare the visual style of the provided [IMAGE] against a detailed textual [DESCRIPTION] of an image's style.

        Objective:
        For each distinct stylistic attribute or aspect mentioned in the [DESCRIPTION], you will evaluate how accurately and consistently the [IMAGE] visually embodies that attribute.

        Scoring Guide:
        Assign a numeric score between 0.0 and 1.0 (inclusive) for each point:

        1.0: Perfect, unequivocal match. The image completely and obviously displays this attribute as described.
        0.8 - 0.9: Strong match with minor, negligible deviations or slight ambiguity.
        0.6 - 0.7: Moderate match. The attribute is present but might have noticeable differences, be less prominent, or only partially apply.
        0.3 - 0.5: Weak match. There are some similarities, but significant discrepancies or outright mismatches.
        0.0 - 0.2: No discernible match. The image contradicts the description for this attribute or it is entirely absent.
        Output Format:
        Your output must be a single JSON object.
        Iterate through the [DESCRIPTION] sequentially. Identify each distinct, actionable stylistic statement (each bullet point or sub-bullet point represents a distinct statement).
        Assign a key style_X where X is an incrementing integer starting from 1 for the very first statement and increasing by 1 for each subsequent statement in the [DESCRIPTION]. The value for each key should be the calculated float score.

        Do NOT include any introductory or concluding text, explanations, or conversational remarks. Output ONLY the JSON object.

        Example of how to number the statements for JSON keys:

        Given this fragment from the description:

        **1. Overall Mood & Atmosphere:**              <-- This becomes `style_1`

        *   Calm and slightly whimsical.
        *   Bright and airy.

        **2. Color Palette:**              <-- This becomes `style_2`

        *   **Dominant Colors:** Gray, beige, and shades of brown.
        *   **Key Accent Colors:** Red (bow tie), white (shirt).
        *   **Color Relationships:** Analogous (browns and beiges). The red and white accents create contrast.
        *   **Saturation:** Muted, with the red as the only strongly saturated color
        *   **Brightness/Value:** Light, with a high overall value. 
        
        Example output:
        {"style_1": 0.9, "style_2": 1.0}
        """
        image_parts = [
            Part.from_image(self._PIL_image_to_vertex_image(image))
            for image in evaluation_images
        ]
        prompts = [
            [comparison_prompt, image_part, description]
            for image_part, description in zip(
                image_parts, reference_image_descriptions
            )
        ]

        with ThreadPoolExecutor(len(evaluation_images)) as executor:
            llm_metrics = dict_mean(
                list(
                    executor.map(
                        self._call_gemini,
                        prompts,
                    )
                )
            )
            return llm_metrics

    def calculate(self, evaluation_images: list[Image]) -> dict:
        """
        Calculate the LLM metrics for a list of evaluation images.

        :param evaluation_images: A list of evaluation images.
        :type evaluation_images: list[Image]
        :return: A dictionary containing the LLM metrics for the evaluation images.
        :rtype: dict
        :raises ValueError: If the number of evaluation images does not match the number of reference images.
        """
        self._verify_input_images(evaluation_images)

        reference_image_descriptions = self._describe_images(
            self.reference_images.values()
        )
        comparison_results = self._compare_images_to_descriptions(
            evaluation_images,
            reference_image_descriptions,
        )
        return comparison_results
